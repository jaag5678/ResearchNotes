\documentclass[notes, xcolor=dvipsnames]{beamer}

\usetheme{Warsaw}

\usepackage{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}

\title{Laws of Order: Expensive Synchronization in Concurrent Algorithms Cannot be Eliminated}
\subtitle{hagit A, Rachid G, Danny H, Petr K, Maged M, Martin V}

\author{Presented by \\ Akshay Gopalakrishnan}

\begin{document}
    
    \begin{frame}

        \maketitle

    \end{frame}


    \begin{frame}{Introduction}
        \begin{itemize}
            \item Designing Concurrent Algorithms with good performance is a non-trivial tasks.
            \item Major part of slowdown is due to synchronization.
            \item Hence efforts are made to remove such costly synchronizations.
            \item This paper shows that it is impossible to remove completely all expensive synchronization primitives for a class of concurrent implementations.
            \item Such a result helps designers to assert when they can stop improving an algorithm this way.
        \end{itemize}
    \end{frame}

    \begin{frame}{Summary of Results}

        The results are based on the usage of atomic Read-after-Write and Write-after-Read operations. 
        The authors show that without using the above primitives, it is: 
        \begin{itemize}
            \item Impossible to build a linearizable implementation that is non-commutative and respects deterministic sequential specification. 
            \item Impossible to build an algorithm that respects mutual exclusion and is deadlock-free. 
        \end{itemize}
        
    \end{frame}

    \begin{frame}{Mutual Exclusion}
        
        Given $N$ processes, each of which accesses a critical section by acquiring a lock, mutual exclusion requires 
        \center{No more than one process can be in the critical section at the same time}

    \end{frame}

    \begin{frame}{Proof intuition}

        Proof by contradiction.
        Part 1:
        \begin{itemize}
            \item Assume that we do not use a shared write updating the lock. 
            \item Then this would imply more than one thread can be in the critical section. 
        \end{itemize}

        Part 2:
        \begin{itemize}
            \item Assume that we do not use ARAW or AWAR as part of the lock implementation. 
            \item Then this would imply more than one thread can read concurrently the same lock and be ready to acquire the lock (update the lock variable).
            \item Then one thread can acquire the lock (by updating the lock variable). 
            \item After which another thread can also acquire the lock (by updating lock variable too!). 
            \item Thu, violating mutual exclusion.
        \end{itemize}

    \end{frame}

    \begin{frame}{Linearizability}

        An algorithm is linearizable w.r.t a sequential specification if
        \center{Each execution of an algorithm is equivalent to some sequential execution of that specification, where the order between non-overlapping methods is preserved.}
        
        Note: Not all linearizable algorithms need to use RAW/WAR. Only some, which have two properties.

    \end{frame}

    \begin{frame}{Two Properties of Linearizable Algorithms that must use RAW/WAR}
        
        \begin{itemize}
            \item Deterministic Sequential Specification - Executing one step of program from same state should yield same result all the time.
            \item Strongly non-commutative methods - Methods whose execution influences further call to the same method.
        \end{itemize}

    \end{frame}

    \begin{frame}{Proof Intuition}

        For this, let us consider a simple example of a set data-structure, which has three methods: 
        \begin{itemize}
            \item \textit{contains(k)} - $\{ ret = (k \in A) \wedge (S=A) \}$.
            \item \textit{remove(k)} - $\{ ret = (k \in A) \wedge (S=A \setminus \{k\}) \}$.
            \item \textit{add(k)} - $\{ ret = (k \notin A) \wedge (S=A \cup \{k\}) \}$
        \end{itemize}

        Firstly note that these methods respect Deterministic Sequential specification. 
        Secondly, note that \textit{add} and \textit{remove} are strongly non-commutative. 
        For instance, if we perform \textit{add(k)} twice, then only one of them will return true, whichever is executed first. 
        Similar reasoning for \textit{remove}.

    \end{frame}

    \begin{frame}{Proof intuition}

        Proof by contradiction:
        \begin{itemize}
            \item Suppose, such a linearizable algorithm does not use RAW or AWAR. 
            \item Given that the methods are strongly non-commutative, they must perform some form of shared write to actually influence a subsequent call to the method. 
            \item And also for them to be influenced by previous method calls, they must also perform a shared read. 
        \end{itemize}

        \begin{itemize}
            \item Now suppose \textit{add(k)} is performed, but not using AWAR or RAW. 
            \item This means, we can have an execution where for instance, the method reads from the set, decides it can write but does not do it. 
            \item Now another \textit{add(k)} does the same thing, but it does its operation fully. Thus writing to the set.
            \item Now the previous call to \textit{add(k)} can be continued to write another $k$ to the set, which is incorrect. 
        \end{itemize}

    \end{frame}
    
    \begin{frame}{Examples of strongly non-commutative methods}

        \begin{itemize}
            \item \textit{add} and \textit{remove} methods in the above set example.
            \item \textit{push} and \textit{pop} method for stack / queue data structures.
            \item \textit{CAS} - Compare and Swap.  
        \end{itemize}
    \end{frame}

    \begin{frame}{Formal proof elements}
        
        \begin{itemize}
            \item Formal language with transition semantics.
            \item Formal definition for executions.
            \item Notion of histories as sub-executions (traces).
            \item Proving both above claims in the same flow using these formal elements. 
        \end{itemize}

    \end{frame}
    
    \begin{frame}{Conclusion}
        
        \begin{itemize}
            \item Utilization of concurrent computation certainly does not come without its downsides.
            \item The use of RAW / AWAR cannot be avoided to ensure properties such as Mutual Exclusion and variants of Linearizability.
        \end{itemize}
        

    \end{frame}

    \begin{frame}{Thank you}
        Questions? 
    \end{frame}

\end{document}