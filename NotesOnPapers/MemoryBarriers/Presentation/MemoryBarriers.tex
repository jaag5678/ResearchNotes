\documentclass[notes, xcolor = dvipsnames]{beamer}

\usetheme{Warsaw}

\usepackage{amsmath}
\usepackage{graphicx}

\title{Memory Barriers: A Hardware view for Software Hackers}
\subtitle{Paul E. McKenney}

\author{Presented by \\ Akshay Gopalakrishnan}

\begin{document}
  
    \begin{frame}
        
        \maketitle
    \end{frame}


    \begin{frame}{Introduction}
        
        \begin{itemize}
            \item This paper represents the inner working of hardware that results in several non-sequential behaviors of our concurrent programs
            \item The paper is rife with examples as well as showcasing the reasons for having such hardware features which in turn help in our programs performing better.
            \item Along with the positives the author also carefully cautions why such rampant changes for performance might result in highly non-trivial behaviors being showcased by the hardware running our programs.
            \item The paper concludes by discussing the then versions of several concurrent hardware that exhibit different non-sequential behaviors.
        \end{itemize}

    \end{frame}

    \begin{frame}{Cache structures}
        
        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.3]{CacheStructure.png}
            }
        \end{figure}

        \begin{itemize}
            \item An extra chunk of memory local to a given cpu (or multiple cpus in bigger systems).
            \item Data read from memory will also be saved in the cache if there is space. 
            \item Any subsequent accesses to the same memory will be first looked at in the cache and if not present, from the main memory.
        \end{itemize}

    \end{frame}

    \begin{frame}{The usefullness of Caches}
        
        \begin{itemize}
            \item Used for faster access of memory. 
            \item Useful when a single shared memory location is being accessed several times but not changed. 
            \item Need to read from main memory which is at least 10x times slower than accessing caches.
            \item Overall performance of program is significantly improved. 
        \end{itemize}

    \end{frame}

    \note{
        In modern systems there are multiple cache levels that exist, each of which is based on the scope. 
        For instance, L1 cache is local to just one core. Whereas L2 is to multiple cores in the same processor (could also be others). 
        All this layering is done for performance, part of the reason why we have such highly non-trivial behaviors of our concurrent programs.
    }

    \begin{frame}{THe MESI protocol}
        
        \begin{itemize}
            \item Multiple caches need to be in synchronization to ensure no stale data in caches exist. 
            \item Caches can communicate with each other via the interconnect network.
            \item We require a protocol to ensure that caches lines are updated accordingly. 
        \end{itemize}

        An example of such a protocol is MESI (Modified Exclusive Shared and Invalid).
        \begin{itemize}
            \item Modified - cache line has the upto date data which resides in memory and the data has been stored by the corresponding CPU. 
            \item Exclusive - cache line is not updated with the recent memory store done by the corresponding CPU. 
            \item Shared  - cache line is in read-only state (CPU needs to consult with other CPU caches before being able to write to it)
            \item Invalid - "empty" cache line and new data can be put here. 
        \end{itemize}


    \end{frame}

    \note{
        Modified state is when my CPU is writiing to memory some data as well as duly updated the cache line. Now this line has the only copy of the latest data in memory. 
        Exlcusive is one step behind modified state, wherein the CPU has updated memory, but not just its cache line. 
        Shared state is when more than one cache line has the same data of memory. This only means, if I want to change one cache line, I must inform the others too. 
        Invalid state can contain any stale data that is never going to be read by the CPU (as it is stale).
    }

    \begin{frame}{MESI Protocol Messages}

        The following messages are passed by a cache line to other caches in the system. 
        \begin{itemize}
            \item Read - Contains the physical address of the cache line to be read. 
            \item Read Response - Contains the data requested by a previous Read message.
            \item Invalidate - Contains the physical address of the cache line to be invalidated. 
            \item Invalidate Acknowledge - CPU receiving an Invalidate message must respond with this message once the specfied cache line is invalidated.
            \item Read Invalidate - Does the action of both Read and Invalidate in one message. 
            \item Writeback - Contains both the address and the data tobe written back to memory (could also implicitly update other cache lines with this).
        \end{itemize}
        
    \end{frame}

    \begin{frame}{Example of Cache Communication}

        The following table represents a sequence of actions done by CPU and the different states (MESI) of the caches after the action has been done. 
        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.2]{MESI_Example.png}
            }
        \end{figure}
        
    \end{frame}

    \begin{frame}{Sequence 0 to 2}

        The 0th sequence represents the default state of cache before being used. 
        Each cache line is set to "Invalid" state. 

        The 1st sequence represents a load done by CPU 0 to fetch data from address 0. 
        The content in stored in CPU0 cache and the state changes to "Shared".

        The 2nd sequences represents a load done by CPU 0 to fethc data from address 0.
        The content is stored in CPU1 cache and the state changes to "Shared".
        
    \end{frame}

    \begin{frame}{Sequence 3 to 5}

        The 3rd sequence represents a load done by CPU 0 to fetch data from address 8.
        This implicity means to Invalidate one's own cache line by sending an invalidate message to it. 
        This gives space to store data from address 0, which now is in the "Shared" state.

        The 4th sequence represents an RMW done by CPU 2 on address 0.
        Before it can do this, it needs to invalidate other caches having data on this address by sending "Invalidate" message.
        Once that is done, it's own cache (CPU 2) is set to "Exclusive" state, while others which had data at address 0 to "Invalid".
        
        The 5th sequence represents the actual store done by CPU 2 (as part of RMW).
        This changes its own cache to state "Modified" and sets the memory of address 0 to "Invalid".
    \end{frame}

    \begin{frame}{Sequence 6 and 7}

        The 6th sequence represents CPU 1 performing an atomic write to data at Address 0.
        Since CPU 2 has its cache in modified state, the increment needs to change that value, while also invalidating their cache. 
        So a "Read Invalidate" message is sent to each cache line, get the updated store value and increment it by 1.
        Now it sets its own cache line (CPU 1) to modified state. 
        While other caches having data at address 0 is set ot invalidate. 

        The 7th sequence represents the actual commiting of the new data to memory. 
        This can either be done by actually issuing a writeback or forcing the cache to make space for other address data (via a Load).
        Here, a Load is issued for address 0 by CPU 1, which forces a writeback to address 0.
        The value at memory address 0 is updated and the state becomes "Valid".
        Meanwhile the cache line of CPU 1 has a "Shared" state.
        
    \end{frame}


    \begin{frame}{Example}

        Consider the example where CPU 0 wants to write to memory whose data is on cache line of CPU 1. 
        This would require the following actions
        \begin{itemize}
            \item CPU 0 sends an Invalidate message to CPU 1.
            \item CPU 1 receives the message, sets the appropriate cache line to "Invalid" state.
            \item CPU 1 sends the Acknowledgement message along with the data on the cache line to CPU 0.
            \item CPU 0 receives the Acknowledgement and data.
            \item CPU 0 does the write to memory and updates its own cache. 
        \end{itemize}

    \end{frame}

    \begin{frame}{Unnecessary stalls due to Writes}

        Notice that in the example above, CPU 0 needs to stall itself until the Acknowledgement message is received from CPU 1.
        This is showcased in the figure below.

        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.6]{Write_Stall.PNG}
            }
        \end{figure}

        CPU 0 need not stall as it will eventually do the write that it is supposed to do. 
        Rather it can continue doing some other task.

    \end{frame}

    \begin{frame}{Solution to write stalling: Write Buffers}
        
        One solution to this is to have a store buffer for each CPU.
        Writes to be done will be stored in this buffer and the CPU can continue doing future tasks.
        When the Acknowledgement messages are received by other CPUs, the write can be committed to its cache line (and eventually to memory).

        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.6]{WriteBuffers.PNG}
            }
        \end{figure}

    \end{frame}

    \begin{frame}{Added Complications}
        
        Adopting the above system does create its own problems. 
        \begin{itemize}
            \item The main cause is that writes to memory committed to store buffers are not read by the same CPU from it.
            \item Instead, the same memory is read either from cache or main memory.
            \item This means, the CPU uses a stale value of memory which results in incorrect execution of programs. 
        \end{itemize}

        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.6]{WriteBufferProblem.PNG}
            }
        \end{figure}

    \end{frame}

    \note{Here is the reason why the example above fails. Consider only CPU 0 and 1 exist.
        \begin{itemize}
            \item CPU 0 wants to do the write $a=1$. So it sends a Read Invalidate message to CPU 1 and commits the store to its Store buffer. 
            \item CPU 1 receives the message and sends the invalidate with the current value of $a$ from its cache. 
            \item CPU 0 meanwhile wants to do $b = a + $, so starts reading $a$ from its own cache line. It receives the value 0. 
            \item Now CPU 0 receives the message from CPU 1 and the store buffer flushes the write to CPU 0 cache line. 
            \item CPU 0 does $b = a + 1$ having read $a=0$ from its cache before.
            \item Now $b=1$.
            \item The assertion fails.
        \end{itemize}
    }
    
    \begin{frame}{Solution seems simple: but?}
        
        The straightforward solution to this is to first let CPUs check their own store buffers during loads from memory and then if not found checking cache and/or memory. 
        However, this also does not prevent all our problems. 
        Consider the code below, where CPU 0 holds exclusive rights to cache line for $b$. 
        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.6]{WriteBufferProblem2.PNG}
            }
        \end{figure}
        The problem is that CPU 0 (if running the first code snippet) can upadte value of $b$ before committing the value of $a$ to cache.
        This may result in the assertion to fail.
    \end{frame}

    \note{
        The reason why this would happen is the following steps
        \begin{itemize}
            \item CPU 0 wants to do $a=1$, but $a$'s cache line is not exclusive/modified, so it sends the write to store buffer and sends Read Invalidate to CPU 1. 
            \item CPU 1 wants to do $while(b==0)$ so tries to read value of $b$. It does not exist in its cache line, so sends a read to CPU 0.  
            \item CPU 0 now wants to do $b=1$ and since it owns this cache line, commits the write immedidately to cache. 
            \item CPU 0 receives the read message from CPU 1 and sends $b=1$ as a response to CPU 1. 
            \item CPU 1 receives the read invalidate for $a$ and sends Acknowledgement along with the value of $a$ to CPU 0. 
            \item CPU 1 then receives the value of $b$. Ends the loop (as $b=1$) and moves to assertion.
            \item The assertion fails. 
            \item CPU 0 receives the Acknowledgement from CPU 1 on $a$. So now it commits from store buffer to cache line.
        \end{itemize}
    }
    
    \begin{frame}{Here comes Write Memory Barriers}

        The problem now is that the hardware does not recognize dependencies between memory accesses (meaning things like conditionals and assertions like the above example).
        At this point, the hardware itself cannot do much.
        
        The solution to this was to have an instruction which specifically tells the hardware that some dependency exists.
        This instruction is called a Write Memory Barrier. 
        The following code with a write barrier solves our issue. 
        \begin{figure}
            \makebox[\textwidth][c]{
                \includegraphics[scale=0.6]{WriteBarrierSol.PNG}
            }
        \end{figure}
        
    \end{frame}

    \note{
        The reason inserting a write memory barrier there solves our problem is that CPU 0 will never do the wrtie $b=1$ until the store buffer holding $a=1$ is first committed to cache. 
    }

    \begin{frame}{Need for Invalidate Queues}
        
    \end{frame}

    \begin{frame}{Added Complications}
        
    \end{frame}

    \begin{frame}{Example}
        
    \end{frame}

    \begin{frame}{Here comes Read Memory Barriers}
        
    \end{frame}

    \begin{frame}{Barrier instructions offered by Linux}
        
    \end{frame}

    \begin{frame}{Hardware Example: Alfa}
        
    \end{frame}

    \begin{frame}{Hardware Example: x86}
    
    \end{frame}

    \begin{frame}{Conclusion}
        
    \end{frame}

\end{document}